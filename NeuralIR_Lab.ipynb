{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural IR"
      ],
      "metadata": {
        "id": "emTl2EjvZaJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhMg_a61tPUC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install whoosh\n",
        "!pip install pytrec_eval\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCAsbfkWtPSI"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wainrob5xE7q"
      },
      "outputs": [],
      "source": [
        "filename = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/lab-data.zip\", \"lab-data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPAKDGnYu5F8"
      },
      "outputs": [],
      "source": [
        "!unzip lab-data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embeddings\n",
        "\n",
        "We will use the pre-trained model TAS-B trained on the MSMARCO dataset. For more information see the following links:\n",
        "\n",
        "\n",
        "\n",
        "*   TAS-B paper: https://arxiv.org/abs/2104.06967\n",
        "*   Pre-trained models on MSMARCO: https://www.sbert.net/docs/pretrained_models.html#msmarco-passage-models\n",
        "\n"
      ],
      "metadata": {
        "id": "44cUnm10gKj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch"
      ],
      "metadata": {
        "id": "YuGHi1ZRa0S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create TAS-B (fine tuned BERT) encoder\n",
        "model_name = \"sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
        "TasB = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "Z220g3Tpa7I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding a phrase\n",
        "doc_a = 'I love IR'\n",
        "\n",
        "doc_a_embedding = TasB.encode(\n",
        "            doc_a,\n",
        "            convert_to_tensor=True,\n",
        "            normalize_embeddings=False\n",
        "        )\n",
        "doc_a_embedding.shape"
      ],
      "metadata": {
        "id": "PfW5Mv8IbAZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding a list of documents\n",
        "doc_b = \"I hate IR\"\n",
        "doc_c = \"IR is alright\"\n",
        "corpus = [doc_a, doc_b, doc_c]\n",
        "\n",
        "corpus_embeddings = TasB.encode(\n",
        "            corpus,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=True,\n",
        "            normalize_embeddings=False\n",
        "        )"
      ],
      "metadata": {
        "id": "sfZqSrBTbHAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings.shape"
      ],
      "metadata": {
        "id": "zON9FTbvbIDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing similarity with embeddings"
      ],
      "metadata": {
        "id": "fU_qi5nZgeJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding a query and searching the corpus\n",
        "q = \"I'm really excited about this IR assignment!\"\n",
        "q_embedding = TasB.encode(\n",
        "            q,\n",
        "            convert_to_tensor=True,\n",
        "            normalize_embeddings=False\n",
        "        )\n",
        "q_embedding.shape"
      ],
      "metadata": {
        "id": "j4s9pUj0bSo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make q_embedding 2-dimensional for matrix multiplication\n",
        "q_embedding = q_embedding.unsqueeze(0)\n",
        "q_embedding.shape"
      ],
      "metadata": {
        "id": "9yAY068LbU8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transpose the corpus embeddings\n",
        "corpus_embeddings_transposed = corpus_embeddings.T\n",
        "\n",
        "# compute dot products between query embedding and transposed corpus embeddings\n",
        "dot_products = torch.mm(q_embedding, corpus_embeddings_transposed)\n",
        "\n",
        "print(dot_products)\n",
        "print(dot_products.shape)"
      ],
      "metadata": {
        "id": "iVVyhTkIbbx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the singleton dimension\n",
        "dot_products = dot_products.squeeze()\n",
        "print(dot_products.shape)"
      ],
      "metadata": {
        "id": "RIjvbxoKbe5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank the documents based on their dot product scores with the query\n",
        "ranked_doc_indices = torch.argsort(dot_products, descending=True)\n",
        "\n",
        "# Display ranked documents\n",
        "print(\"Query:\", q)\n",
        "print(\"\\nTop documents:\")\n",
        "\n",
        "for i in ranked_doc_indices:\n",
        "    print(corpus[i], \"(Score:\", dot_products[i].item(), \")\")"
      ],
      "metadata": {
        "id": "7ElyOFnXbn5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing and reading document embeddings"
      ],
      "metadata": {
        "id": "YU2eNmARgmrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "IPBWO7zsce50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids = ['A','B','C']"
      ],
      "metadata": {
        "id": "R8CwHqE3bqqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with doc_ids as keys and embeddings as values\n",
        "# we convert our tensors to lists so that we can save to a json file\n",
        "dict_corpus_embeddings=dict(zip(doc_ids,corpus_embeddings.tolist()))"
      ],
      "metadata": {
        "id": "8Z-Uro3IcKrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save embeddings to json\n",
        "json.dump(dict_corpus_embeddings,open('corpus_embeddings.json','w'))"
      ],
      "metadata": {
        "id": "QXnCHLN-cScE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read embeddings from json\n",
        "dict_corpus_embeddings_2 = json.load(open('corpus_embeddings.json','r'))\n",
        "type(dict_corpus_embeddings_2)"
      ],
      "metadata": {
        "id": "BXKSxSEMew15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert dict of embeddings to pytorch tensor\n",
        "corpus_embeddings_2 = torch.Tensor([dict_corpus_embeddings_2[doc] for doc in doc_ids])"
      ],
      "metadata": {
        "id": "TqZ7vFqBfTuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check to make sure saving and reading embeddings using json did not change them\n",
        "(corpus_embeddings_2==corpus_embeddings).all()"
      ],
      "metadata": {
        "id": "DYvX4LmNflbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leverage pandas vectorization to read from json\n",
        "import pandas as pd\n",
        "corpus_embeddings_3 = torch.Tensor(pd.read_json('corpus_embeddings.json')[doc_ids].T.values)\n",
        "(corpus_embeddings_3==corpus_embeddings).all()"
      ],
      "metadata": {
        "id": "ODtwFV-faeV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TREC_EVAL with neural IR"
      ],
      "metadata": {
        "id": "X11r8x4BrMtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh import index, writing\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import subprocess\n",
        "import pytrec_eval\n",
        "from whoosh import qparser"
      ],
      "metadata": {
        "id": "z_bwdwj0qpC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralResults():\n",
        "  '''\n",
        "  This class is used to rerank documents returned by whoosh in an interface that\n",
        "  imitates whoosh.searching.Results (the datatype of topicResults in pyTrecEval)\n",
        "  '''\n",
        "  def __init__(self, booleansearchdocs,scores,rankings, file_list):\n",
        "    self.results=[]\n",
        "    for idx in rankings:\n",
        "      self.results.append({'file_path':file_list[booleansearchdocs[idx]],'score':scores[idx] })\n",
        "\n",
        "  def score(self,docnum):\n",
        "    return self.results[docnum]['score']\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.results.__iter__()\n",
        "\n",
        "class IRSystem():\n",
        "\n",
        "    def __init__(self, data_dir=\"lab-data\"):\n",
        "        self.topic_file = os.path.join(data_dir, \"air.topics\")\n",
        "        self.qrels_file = os.path.join(data_dir, \"air.qrels\")\n",
        "        self.document_dir = os.path.join(data_dir, \"documents\")\n",
        "        self.file_list = [str(filePath) for filePath in Path(self.document_dir).glob(\"**/*\") if filePath.is_file()]\n",
        "\n",
        "        self.create_index()\n",
        "        self.create_parser_searcher()\n",
        "\n",
        "        self.neuralrerank=True\n",
        "\n",
        "    def set_neural_rerank(self,val):\n",
        "        self.neuralrerank=val\n",
        "\n",
        "    def create_index(self):\n",
        "        analyzer =RegexTokenizer() | LowercaseFilter()  |  StopFilter() | StemFilter()\n",
        "        schema = Schema(file_path = ID(stored=True), file_content = TEXT(analyzer = analyzer))\n",
        "        indexDir = tempfile.mkdtemp()\n",
        "        self.index_sys = index.create_in(indexDir, schema)\n",
        "\n",
        "\n",
        "    def add_files(self):\n",
        "        writer = writing.BufferedWriter(self.index_sys, period=None, limit=1000)\n",
        "        file_contents=[]\n",
        "        try:\n",
        "            # write each file to index\n",
        "            for docNum, filePath in enumerate(self.file_list):\n",
        "                with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    fileContent = f.read()\n",
        "                    writer.add_document(file_path = filePath, file_content = fileContent)\n",
        "                    file_contents.append(fileContent)\n",
        "                    # print status every 1000 documents\n",
        "                    if (docNum+1) % 1000 == 0:\n",
        "                        print(\"already indexed:\", docNum+1)\n",
        "            print(\"done indexing.\")\n",
        "\n",
        "        finally:\n",
        "            # close the index\n",
        "            writer.close()\n",
        "        print(\"Computing Embeddings\")\n",
        "        self.TasB = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n",
        "        self.corpus_embeddings=self.TasB.encode(\n",
        "                                          file_contents,\n",
        "                                          convert_to_tensor=True,\n",
        "                                          show_progress_bar=True,\n",
        "                                          normalize_embeddings=True\n",
        "                                      )\n",
        "\n",
        "    def create_parser_searcher(self):\n",
        "        self.query_parser = QueryParser(\"file_content\", schema=self.index_sys.schema, group=qparser.OrGroup)\n",
        "        self.searcher = self.index_sys.searcher()\n",
        "\n",
        "\n",
        "\n",
        "    def perform_search(self, topic_phrase):\n",
        "        topicResults = self.searcher.search(self.query_parser.parse(topic_phrase), limit=None) #regular whoosh search\n",
        "        if self.neuralrerank: #if we want to rerank the retrieved documents\n",
        "          booleansearchdocs = list(topicResults.docs()) #get the retrieved docs\n",
        "          if len(booleansearchdocs)<=1: #if 0 or 1 retrieved docs then reranking is not necessary\n",
        "            return topicResults\n",
        "          query_embedding = self.TasB.encode(topic_phrase,convert_to_tensor=True, normalize_embeddings=True) #embed querry\n",
        "          scores = torch.mm(query_embedding.unsqueeze(0), self.corpus_embeddings[booleansearchdocs].T).squeeze() #compute dot product between querry embedding and document embeddings of the returned docs\n",
        "          rankings = torch.argsort(scores,descending=True) #get order of scores by index\n",
        "          return NeuralResults(booleansearchdocs,scores, rankings, self.file_list)\n",
        "        else:\n",
        "          return topicResults\n",
        "\n",
        "    @staticmethod\n",
        "    def post_process_score(score):\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def print_trec_eval_result(results):\n",
        "\n",
        "        if not results:\n",
        "            print('empty results')\n",
        "            return\n",
        "\n",
        "        def print_line(name, scope, num):\n",
        "            print('{:25s}{:8s}{:.4f}'.format(name, scope, num))\n",
        "\n",
        "        for query_id, query_measures in results.items():\n",
        "            for measure, value in query_measures.items():\n",
        "                if measure == \"runid\":\n",
        "                    continue\n",
        "                print_line(measure, query_id, value)\n",
        "\n",
        "        for measure in query_measures.keys():\n",
        "            if measure == \"runid\":\n",
        "                continue\n",
        "            print_line(\n",
        "                measure,\n",
        "                'all',\n",
        "                pytrec_eval.compute_aggregated_measure(\n",
        "                    measure,\n",
        "                    [query_measures[measure]\n",
        "                     for query_measures in results.values()]))\n",
        "\n",
        "\n",
        "    def score(self,docnum,topic_results):\n",
        "        return topic_results.score(docnum)\n",
        "\n",
        "\n",
        "    def print_rel_name(self, q_id):\n",
        "        with open(self.topic_file, \"r\") as tf:\n",
        "            topics = tf.read().splitlines()\n",
        "        for topic in topics:\n",
        "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "            if topic_id == q_id:\n",
        "                print(\"---------------------------Topic_id and Topic_phrase----------------------------------\")\n",
        "                print(topic_id, topic_phrase)\n",
        "                 # get search result\n",
        "                topic_results = self.perform_search(topic_phrase)\n",
        "                print(\"---------------------------Return documents----------------------------------\")\n",
        "                for (docnum, result) in enumerate(topic_results):\n",
        "                    score = self.score(docnum, topic_results, topic_phrase)\n",
        "                    score = self.post_process_score(score)\n",
        "                    print(\"%s Q0 %s %d %lf test\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                print(\"---------------------------Relevant documents----------------------------------\")\n",
        "                with open(self.qrels_file, 'r') as f_qrel:\n",
        "                    qrels = f_qrel.readlines()\n",
        "                    for i in qrels:\n",
        "                        qid, _, doc, rel = i.rstrip().split(\" \")\n",
        "                        if qid == q_id and rel == \"1\":\n",
        "                            print(i.rstrip())\n",
        "\n",
        "    def py_trec_eval(self):\n",
        "\n",
        "        self.create_parser_searcher()\n",
        "        # Load topic file - a list of topics(search phrases) used for evalutation\n",
        "        with open(self.topic_file, \"r\") as tf:\n",
        "            topics = tf.read().splitlines()\n",
        "\n",
        "            # create an output file to which we'll write our results\n",
        "        temp_output_file = tempfile.mkstemp()[1]\n",
        "        with open(temp_output_file, \"w\") as outputTRECFile:\n",
        "            # for each evaluated topic:\n",
        "            # build a query and record the results in the file in TREC_EVAL format\n",
        "            for topic in topics:\n",
        "                topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "                print(topic_id)\n",
        "                # get search result\n",
        "                topic_results = self.perform_search(topic_phrase)\n",
        "                # format the result\n",
        "                for (docnum, result) in enumerate(topic_results):\n",
        "                    score = self.score(docnum, topic_results)\n",
        "                    outputTRECFile.write(\n",
        "                        \"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                    topic_with_result = topic_id\n",
        "\n",
        "\n",
        "        with open(self.qrels_file, 'r') as f_qrel:\n",
        "            qrel = pytrec_eval.parse_qrel(f_qrel)\n",
        "\n",
        "        with open(temp_output_file, 'r') as f_run:\n",
        "            run = pytrec_eval.parse_run(f_run)\n",
        "\n",
        "        evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "            qrel, pytrec_eval.supported_measures)\n",
        "\n",
        "        results = evaluator.evaluate(run)\n",
        "\n",
        "        #fill results dictionary with queries that returned 0 documents\n",
        "        topic_ids = {t.split()[0] for t in topics}\n",
        "        for emptyresult_topicid in topic_ids.difference(set(results.keys())):\n",
        "            num_rel = float(sum(qrel[emptyresult_topicid].values()))\n",
        "            if num_rel>0:\n",
        "              topic_stats={measure:0.0 for measure in results[topic_with_result]}\n",
        "            else:\n",
        "              topic_stats={measure:1.0 for measure in results[topic_with_result]}\n",
        "            topic_stats[\"num_rel\"]=num_rel\n",
        "            topic_stats[\"num_ret\"] = 0.0\n",
        "            topic_stats[\"num_rel_ret\"] = 0.0\n",
        "            topic_stats[\"num_q\"]=1.0\n",
        "\n",
        "            results[emptyresult_topicid] = topic_stats\n",
        "\n",
        "\n",
        "        self.print_trec_eval_result(results)\n"
      ],
      "metadata": {
        "id": "FAtQ4A3gruTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irsystem = IRSystem(\"lab-data\")\n",
        "irsystem.add_files()"
      ],
      "metadata": {
        "id": "o6RlnOpUuDMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "irsystem.set_neural_rerank(True)\n",
        "irsystem.py_trec_eval()"
      ],
      "metadata": {
        "id": "sWAmdu-8uF-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irsystem.set_neural_rerank(False)\n",
        "irsystem.py_trec_eval()"
      ],
      "metadata": {
        "id": "qu9tk8vXzlqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On this dataset, reranking using TAS-B decreased our MAP from 0.75 to 0.68.\n",
        "\n",
        "Here are some resources on how to achieve better performance with Neural IR:\n",
        "\n",
        "\n",
        "*   [Pretrained Transformers for Text Ranking:BERT and Beyond](https://arxiv.org/abs/2010.06467)\n",
        "*   [SPLADE methodology for contextual query reformulation/expansion](https://www.pinecone.io/learn/splade/)\n",
        "\n"
      ],
      "metadata": {
        "id": "7XVQF-akz-88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example code to create embeddings in batches\n",
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.document_dir = os.path.join(data_dir, \"documents\")\n",
        "        self.data = [str(filePath) for filePath in Path(self.document_dir).glob(\"**/*\") if filePath.is_file()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return open(self.data[idx], \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "data_loader = DataLoader(CustomDataset(\"lab-data\"), batch_size=128, shuffle=False)\n",
        "embeddings=[]\n",
        "for data in data_loader:\n",
        "  e=TasB.encode(\n",
        "                                                        data,\n",
        "                                                        convert_to_tensor=True,\n",
        "                                                    )\n",
        "  embeddings.append(e)\n",
        "embeddings = torch.cat(embeddings, axis=0)"
      ],
      "metadata": {
        "id": "YIjKsY8_zolI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
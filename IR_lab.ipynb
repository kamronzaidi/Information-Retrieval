{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsLfOVcOtPSH"
      },
      "source": [
        "# IR Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkl3Xh-h2IBd"
      },
      "source": [
        "## Install, import modules and download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1iecxlriuhuA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whoosh in c:\\users\\kirby\\anaconda3\\lib\\site-packages (2.7.4)\n",
            "Requirement already satisfied: pytrec-eval-terrier in c:\\users\\kirby\\anaconda3\\lib\\site-packages (0.5.5)\n",
            "Requirement already satisfied: wget in c:\\users\\kirby\\anaconda3\\lib\\site-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install whoosh\n",
        "!pip install pytrec-eval-terrier\n",
        "!pip install wget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MCAsbfkWtPSI"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nltk'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33012/3826303664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytrec_eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'omw-1.4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
          ]
        }
      ],
      "source": [
        "from whoosh import index, writing\n",
        "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
        "from whoosh.analysis import *\n",
        "from whoosh.qparser import QueryParser\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import subprocess\n",
        "import pytrec_eval\n",
        "import wget\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wainrob5xE7q"
      },
      "outputs": [],
      "source": [
        "filename = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/lab-data.zip\", \"lab-data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPAKDGnYu5F8"
      },
      "outputs": [],
      "source": [
        "!unzip lab-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hu7KjtPtPSL"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"lab-data\"\n",
        "DOCUMENTS_DIR = os.path.join(DATA_DIR, \"documents\")\n",
        "TOPIC_FILE = os.path.join(DATA_DIR, \"air.topics\")\n",
        "QRELS_FILE = os.path.join(DATA_DIR, \"air.qrels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kifl9NW4tPSO"
      },
      "source": [
        "## Part 1: Basic Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMkIZH6wtPSP"
      },
      "source": [
        "### Creating the index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LddgarTcFnGS"
      },
      "source": [
        "To begin using Whoosh, you need an index object. The first time you create an index, you must define the indexâ€™s schema. The schema lists the fields in the index. A field is a piece of information for each document in the index, such as its title or text content. A field can be indexed (meaning it can be searched) and/or stored (meaning the value that gets indexed is returned with the results; this is useful for fields such as the title).\n",
        "\n",
        "More information:\n",
        "https://whoosh.readthedocs.io/en/latest/schema.html?highlight=schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqMbeQ-otPSR"
      },
      "outputs": [],
      "source": [
        "def createIndex(schema):\n",
        "    # Generate a temporary directory for the index\n",
        "    indexDir = tempfile.mkdtemp()\n",
        "\n",
        "    # create and return the index\n",
        "    return index.create_in(indexDir, schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFpIvO8NtPST"
      },
      "outputs": [],
      "source": [
        "# first, define a Schema for the index\n",
        "mySchema = Schema(file_path = ID(stored=True),\n",
        "                  file_content = TEXT(analyzer = RegexTokenizer()))\n",
        "\n",
        "# now, create the index at the path INDEX_DIR based on the new schema\n",
        "myIndex = createIndex(mySchema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COQ5Pn_ntPSW"
      },
      "source": [
        "### Indexing the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBBnPXM0tPSW"
      },
      "outputs": [],
      "source": [
        "def addFilesToIndex(indexObj, fileList):\n",
        "    # open writer\n",
        "    writer = writing.BufferedWriter(indexObj, period=None, limit=1000)\n",
        "\n",
        "    try:\n",
        "        # write each file to index\n",
        "        for docNum, filePath in enumerate(fileList):\n",
        "            with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
        "                fileContent = f.read()\n",
        "                writer.add_document(file_path = filePath,\n",
        "                                    file_content = fileContent)\n",
        "\n",
        "                # print status every 1000 documents\n",
        "                if (docNum+1) % 1000 == 0:\n",
        "                    print(\"already indexed:\", docNum+1)\n",
        "        print(\"done indexing.\")\n",
        "\n",
        "    finally:\n",
        "        # close the index\n",
        "        writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60nMMFAtPSZ"
      },
      "outputs": [],
      "source": [
        "# Build a list of files to index\n",
        "filesToIndex = [str(filePath) for filePath in Path(DOCUMENTS_DIR).glob(\"**/*\") if filePath.is_file()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipB2OTnbtPSb"
      },
      "outputs": [],
      "source": [
        "# Check the list\n",
        "filesToIndex[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DfEVecutPSd"
      },
      "outputs": [],
      "source": [
        "# count files to index\n",
        "print(\"number of files:\", len(filesToIndex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGbkPnCxtPSh"
      },
      "outputs": [],
      "source": [
        "addFilesToIndex(myIndex, filesToIndex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4w4Xs8htPSm"
      },
      "source": [
        "### Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvvTIO9BJAF5"
      },
      "source": [
        "More information: https://whoosh.readthedocs.io/en/latest/api/qparser.html?highlight=queryparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-myzyp8tPSn"
      },
      "outputs": [],
      "source": [
        "# define a query parser for the field \"file_content\" in the index\n",
        "myQueryParser = QueryParser(\"file_content\", schema=myIndex.schema)\n",
        "mySearcher = myIndex.searcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x9Bm1F4tPSr"
      },
      "outputs": [],
      "source": [
        "# run a sample query for the phrase \"duck\"\n",
        "sampleQuery = myQueryParser.parse(\"duck\")\n",
        "sampleQueryResults = mySearcher.search(sampleQuery, limit=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE0g4G9ZJbdY"
      },
      "outputs": [],
      "source": [
        "sampleQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSZobtNCJpvE"
      },
      "outputs": [],
      "source": [
        "sampleQueryResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtHIowDvJorC"
      },
      "outputs": [],
      "source": [
        "# inspect the result:\n",
        "# for each document print the rank and the score\n",
        "for (docnum, result) in enumerate(sampleQueryResults):\n",
        "    score = sampleQueryResults.score(docnum)\n",
        "    fileName = os.path.basename(result[\"file_path\"])\n",
        "    print(fileName, docnum, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCOEN38qD2gU"
      },
      "outputs": [],
      "source": [
        "sampleQueryResults.docs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6LqtfpJtPSt"
      },
      "source": [
        "### Evaluation using TREC_EVAL\n",
        "In order to evaluate our results we will use a topic file - a list of topics we use to evaluate our IR system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SphLDVwKtPSt"
      },
      "outputs": [],
      "source": [
        "# print the topic file\n",
        "with open(TOPIC_FILE, \"r\") as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_OU8GB7tPSw"
      },
      "source": [
        "We will compare our evaluate our results with a set of judged results(qrels file) using TREC_EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Y-OiZltPSw"
      },
      "outputs": [],
      "source": [
        "# print the first 10 lines in the qrels file\n",
        "with open(QRELS_FILE, \"r\") as f:\n",
        "    qrels10 = f.readlines()[:10]\n",
        "    print(\"\".join(qrels10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQrUM9htPSz"
      },
      "source": [
        "The follwing function takes a topic file, a qrels file, a query parser and a searcher and use pytrec_eval to compare our results with the provided qrels file (see assignment PDF for more details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOl24QqnRvQY"
      },
      "outputs": [],
      "source": [
        "def pyTrecEval(topicFile, qrelsFile, queryParser, searcher):\n",
        "    # Load topic file - a list of topics(search phrases) used for evalutation\n",
        "    with open(topicFile, \"r\") as tf:\n",
        "        topics = tf.read().splitlines()\n",
        "\n",
        "    # create an output file to which we'll write our results\n",
        "    tempOutputFile = tempfile.mkstemp()[1]\n",
        "    with open(tempOutputFile, \"w\") as outputTRECFile:\n",
        "        # for each evaluated topic:\n",
        "        # build a query and record the results in the file in TREC_EVAL format\n",
        "        for topic in topics:\n",
        "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "            #print(topic_id, topic_phrase)\n",
        "            topicQuery = queryParser.parse(topic_phrase)\n",
        "            topicResults = searcher.search(topicQuery, limit=None)\n",
        "            for (docnum, result) in enumerate(topicResults):\n",
        "                score = topicResults.score(docnum)\n",
        "                #print(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "                topic_with_result = topic_id\n",
        "\n",
        "    with open(qrelsFile, 'r') as f_qrel:\n",
        "        qrel = pytrec_eval.parse_qrel(f_qrel)\n",
        "\n",
        "    with open(tempOutputFile, 'r') as f_run:\n",
        "        run = pytrec_eval.parse_run(f_run)\n",
        "\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        qrel, pytrec_eval.supported_measures)\n",
        "\n",
        "    results = evaluator.evaluate(run)\n",
        "\n",
        "    #fill results dictionary with queries that were returned 0 documents\n",
        "    topic_ids = {t.split()[0] for t in topics}\n",
        "    for emptyresult_topicid in topic_ids.difference(set(results.keys())):\n",
        "        num_rel = float(sum(qrel[emptyresult_topicid].values()))\n",
        "        if num_rel>0:\n",
        "          topic_stats={measure:0.0 for measure in results[topic_with_result]}\n",
        "        else:\n",
        "          topic_stats={measure:1.0 for measure in results[topic_with_result]}\n",
        "        topic_stats[\"num_rel\"]=num_rel\n",
        "        topic_stats[\"num_ret\"] = 0.0\n",
        "        topic_stats[\"num_rel_ret\"] = 0.0\n",
        "        topic_stats[\"num_q\"]=1.0\n",
        "\n",
        "        results[emptyresult_topicid] = topic_stats\n",
        "\n",
        "    def print_line(measure, scope, value):\n",
        "        print('{:25s}{:8s}{:.4f}'.format(measure, scope, value))\n",
        "\n",
        "    for query_id, query_measures in results.items():\n",
        "        for measure, value in query_measures.items():\n",
        "            if measure == \"runid\":\n",
        "              continue\n",
        "            print_line(measure, query_id, value)\n",
        "    for measure in query_measures.keys():\n",
        "        if measure == \"runid\":\n",
        "              continue\n",
        "        print_line(\n",
        "            measure,\n",
        "            'all',\n",
        "            pytrec_eval.compute_aggregated_measure(\n",
        "                measure,\n",
        "                [query_measures[measure]\n",
        "                 for query_measures in results.values()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz9wA8WbSHCE"
      },
      "outputs": [],
      "source": [
        "pyTrecEval(TOPIC_FILE, QRELS_FILE, myQueryParser, mySearcher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7LMEW83xtiB"
      },
      "outputs": [],
      "source": [
        "def printRelName(topicFile, qrelsFile, queryParser, searcher, id):\n",
        "  with open(topicFile, \"r\") as tf:\n",
        "        topics = tf.read().splitlines()\n",
        "  for topic in topics:\n",
        "        topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
        "        if topic_id == id:\n",
        "          print(\"---------------------------Topic_id and Topic_phrase----------------------------------\")\n",
        "          print(topic_id, topic_phrase)\n",
        "          topicQuery = queryParser.parse(topic_phrase)\n",
        "          topicResults = searcher.search(topicQuery, limit=None)\n",
        "          print(\"---------------------------Return documents----------------------------------\")\n",
        "          for (docnum, result) in enumerate(topicResults):\n",
        "              score = topicResults.score(docnum)\n",
        "              print(\"%s Q0 %s %d %lf test\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
        "          print(\"---------------------------Relevant documents----------------------------------\")\n",
        "          with open(qrelsFile, 'r') as f_qrel:\n",
        "            qrels = f_qrel.readlines()\n",
        "            for i in qrels:\n",
        "              qid, _, doc, rel = i.rstrip().split(\" \")\n",
        "              if qid == id and rel == \"1\":\n",
        "                print(i.rstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nshKlvO8xv4g"
      },
      "outputs": [],
      "source": [
        "printRelName(TOPIC_FILE, QRELS_FILE, myQueryParser, mySearcher, \"01\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hehQMz-NtPS6"
      },
      "source": [
        "## Part 2: Evaluating different configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRb5SpIdtPS7"
      },
      "source": [
        "### Inspecting our index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yimuRBkqtPS8"
      },
      "outputs": [],
      "source": [
        "# Is it empty?\n",
        "print(\"Index is empty?\", myIndex.is_empty())\n",
        "\n",
        "# How many files indexed?\n",
        "print(\"Number of indexed files:\", myIndex.doc_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVgYyGCXtPS-"
      },
      "outputs": [],
      "source": [
        "# define a reader object on the index\n",
        "myReader = myIndex.reader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PdtE24ctPTD"
      },
      "outputs": [],
      "source": [
        "# print first 5 indexed documents\n",
        "[(docnum, doc_dict) for (docnum, doc_dict) in myReader.iter_docs()][0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhVLFv81tPTH"
      },
      "outputs": [],
      "source": [
        "# list indexed terms for field \"file_content\"\n",
        "[term for term in myReader.field_terms(\"file_content\")][1000:1025]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_KS7TeztPTK"
      },
      "outputs": [],
      "source": [
        "#how many terms do we have?\n",
        "print(myReader.field_length(\"file_content\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCcAc3zltPTM"
      },
      "outputs": [],
      "source": [
        "# how many documents have the phares \"bit\", blob\"\n",
        "#   in the field \"file_content\"?\n",
        "print(\"# docs with 'bit'\", myReader.doc_frequency(\"file_content\", \"bit\"))\n",
        "print(\"# docs with 'are'\", myReader.doc_frequency(\"file_content\", \"are\"))\n",
        "print(\"# docs with 'get'\", myReader.doc_frequency(\"file_content\", \"get\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RtEaMltPTO"
      },
      "source": [
        "### Text Analyzers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MryBFZMftPTO"
      },
      "outputs": [],
      "source": [
        "# we start with basic tokenizer\n",
        "tokenizer = RegexTokenizer()\n",
        "[token.text for token in tokenizer(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAqeToZmtPTS"
      },
      "outputs": [],
      "source": [
        "# we might want use stemming:\n",
        "stmAnalyzer = RegexTokenizer() | StemFilter()\n",
        "[token.text for token in stmAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYtFoYcQtPTX"
      },
      "outputs": [],
      "source": [
        "# We probably want to lower-case it\n",
        "# so we add LowercaseFilter\n",
        "stmLwrAnalyzer = RegexTokenizer() | LowercaseFilter() | StemFilter()\n",
        "[token.text for token in stmLwrAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PolzmL-ftPTZ"
      },
      "outputs": [],
      "source": [
        "# we probably want to ignore words like \"we\", \"are\", \"with\" when we index files\n",
        "# so we add StopFilter to filter stop words\n",
        "stmLwrStpAnalyzer = RegexTokenizer() | LowercaseFilter() | StopFilter() | StemFilter()\n",
        "[token.text for token in stmLwrStpAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1lCwGmtPTb"
      },
      "outputs": [],
      "source": [
        "# we also probably want to break phrases like \"whoosh.analysis\" into \"whoosh\" and \"analysis\"\n",
        "# so we add IntraWordFilter\n",
        "stmLwrStpIntraAnalyzer = RegexTokenizer() | LowercaseFilter() | IntraWordFilter() | StopFilter() | StemFilter()\n",
        "[token.text for token in stmLwrStpIntraAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_8GzxX0tPTd"
      },
      "source": [
        "### Evaluating the new analyzers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FlXm9UotPTd"
      },
      "outputs": [],
      "source": [
        "# define a Schema with the new analyzer\n",
        "mySchema2 = Schema(file_path = ID(stored=True),\n",
        "                   file_content = TEXT(analyzer = stmLwrStpIntraAnalyzer))\n",
        "\n",
        "# create the index based on the new schema\n",
        "myIndex2 = createIndex(mySchema2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52MlF85LtPTh"
      },
      "outputs": [],
      "source": [
        "addFilesToIndex(myIndex2, filesToIndex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chy78JJ2tPTk"
      },
      "outputs": [],
      "source": [
        "# define a query parser for the field \"file_content\" in the index\n",
        "myQueryParser2 = QueryParser(\"file_content\", schema=myIndex2.schema)\n",
        "mySearcher2 = myIndex2.searcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHI_RkXstPTn"
      },
      "outputs": [],
      "source": [
        "pyTrecEval(TOPIC_FILE, QRELS_FILE, myQueryParser2, mySearcher2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQa08KoStPTp"
      },
      "outputs": [],
      "source": [
        "# let count the same words again\n",
        "myReader2 = myIndex2.reader()\n",
        "print(\"# docs with 'bit'\", myReader2.doc_frequency(\"file_content\", \"bit\"))\n",
        "print(\"# docs with 'are'\", myReader2.doc_frequency(\"file_content\", \"are\"))\n",
        "print(\"# docs with 'get'\", myReader2.doc_frequency(\"file_content\", \"get\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286mQai_tPTr"
      },
      "source": [
        "**Can you explain the differences?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHxWeV1ltPTr"
      },
      "source": [
        "### Using NLTK's stemmers and lemmatizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QN89h1rtPTs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tAOTINetPTt"
      },
      "outputs": [],
      "source": [
        "# download required resources\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47PMWb54tPTv"
      },
      "outputs": [],
      "source": [
        "# we'll compare two stemmers and a lemmatizer\n",
        "lrStem = LancasterStemmer()\n",
        "sbStem = SnowballStemmer(\"english\")\n",
        "wnLemm = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC01OAIstPTx"
      },
      "outputs": [],
      "source": [
        "# define a list of words to compare the stemmers on\n",
        "listWords = [\"going\", \"saying\", \"minimize\", \"maximum\",\n",
        "             \"meeting\", \"files\", \"tries\", \"is\", \"are\", \"beautiful\",\n",
        "             \"summarize\", \"better\", \"dogs\", \"phenomena\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dei5v9GrtPT0"
      },
      "outputs": [],
      "source": [
        "for word in listWords:\n",
        "    print(\"%15s %15s %15s %15s\" % (lrStem.stem(word),\n",
        "                                   sbStem.stem(word),\n",
        "                                   wnLemm.lemmatize(word),\n",
        "                                   wnLemm.lemmatize(word, 'v')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6BmsPrntPT3"
      },
      "source": [
        "### How to use NLTK stemmers / lemmatizers in Whoosh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXumu_JztPT4"
      },
      "outputs": [],
      "source": [
        "# Dont change this! Use it as-is in your code\n",
        "# This filter will run for both the index and the query\n",
        "from whoosh.analysis import Filter\n",
        "class CustomFilter(Filter):\n",
        "    is_morph = True\n",
        "    def __init__(self, filterFunc, *args, **kwargs):\n",
        "        self.customFilter = filterFunc\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "    def __eq__(self):\n",
        "        return (other\n",
        "                and self.__class__ is other.__class__)\n",
        "    def __call__(self, tokens):\n",
        "        for t in tokens:\n",
        "            if t.mode == 'query': # if called by query parser\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t\n",
        "            else: # == 'index' if called by indexer\n",
        "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
        "                yield t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q50AgsrmtPT5"
      },
      "outputs": [],
      "source": [
        "# Example1: Whoosh filter for NLTK's LancasterStemmer\n",
        "myFilter1 = RegexTokenizer() | CustomFilter(LancasterStemmer().stem)\n",
        "[token.text for token in myFilter1(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUWjmhjZtPT7"
      },
      "outputs": [],
      "source": [
        "# Example2: Whoosh filter for NLTK's WordNetLemmatizer\n",
        "myFilter2 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize)\n",
        "[token.text for token in myFilter2(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okNoDllntPT8"
      },
      "outputs": [],
      "source": [
        "# Example3: Whoosh filter for NLTK's WordNetLemmatizer for verbs\n",
        "myFilter3 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize, 'v')\n",
        "[token.text for token in myFilter3(\"We are going to do Text Analysis with whoosh.analysis\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruhos_p-tPUB"
      },
      "source": [
        "You can now use myFilter1/2/3 as part of your Schema\n",
        "\n",
        "------------\n",
        "You can find details of other NLTK Stemmers and Lemmatizers here:\n",
        "\n",
        "http://www.nltk.org/api/nltk.stem.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
